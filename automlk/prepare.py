import pickle
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold
from .dataset import *
from .specific import *


def get_eval_sets(dataset_id):
    """
    get the complete eval set

    :param dataset_id: id of the dataset
    :return: eval set (XySet object)
    """
    return pickle.load(open(get_dataset_folder(dataset_id) + '/data/eval_set.pkl', 'rb'))


def get_idx_train_test(dataset_id):
    """
    get the index of the train and test set from the original train set

    :param dataset_id: id of the dataset
    :return: train index, test index
    """
    return pickle.load(open(get_dataset_folder(dataset_id) + '/data/i_train_test.pkl', 'rb'))


def prepare_dataset_sets(dt):
    """
    creates the train & test set, and the evaluation set per folds

    :param dt: dataset
    :return:
    """
    # create train & test set
    X, y, X_train, X_test, y_train, y_test, X_submit, id_submit, i_train, i_test = __create_train_test(dt)

    # prepare y values
    y, y_train, y_test = __prepare_y(dt, y, y_train, y_test)

    # create cv folds
    cv_folds = __create_cv(dt, X_train, y_train)

    # prepare and store eval set
    y_eval_list, y_eval, idx_eval, idx_eval0 = __store_eval_set(dt, y_train, y_test, cv_folds)

    # then store all these results in a pickle store
    ds = XySet(X, y, X_train, y_train, X_test, y_test, X_submit, id_submit, cv_folds, y_eval_list, y_eval, idx_eval, idx_eval0)
    pickle.dump(ds, open(get_dataset_folder(dt.dataset_id) + '/data/eval_set.pkl', 'wb'))

    # and keep index of split for future use (eg match predictions with initial file)
    pickle.dump([i_train, i_test], open(get_dataset_folder(dt.dataset_id) + '/data/i_train_test.pkl', 'wb'))


def __create_train_test(dataset):
    dataset = get_dataset(dataset.dataset_id)
    # feature engineering
    fe = get_feature_engineering(dataset.dataset_id)
    data_train = dataset.get_data()
    if fe != '':
        data_train = apply_feature_engineering(dataset.dataset_id, data_train)

    # update columns
    dataset.update_features(data_train)
    dataset.update_calc()
    dataset.save(dataset.dataset_id)

    # split into train & test set
    if dataset.with_test_set:
        data_test = dataset.get_data('test')
        if fe != '':
            data_test = apply_feature_engineering(dataset.dataset_id, data_test)

        X_train = data_train[dataset.x_cols]
        y_train = data_train[dataset.y_col].values

        X_test = data_test[dataset.x_cols]
        y_test = data_test[dataset.y_col].values

        X = X_train.copy()
        y = y_train.copy()
        i_train = list(range(len(data_train)))
        i_test = []
    else:
        X = data_train[dataset.x_cols]
        y = data_train[dataset.y_col].values
        val_col = dataset.val_col
        if val_col != 'index':
            val_values = np.sort(X[val_col].unique())
            val_train, val_test = train_test_split(val_values, test_size=dataset.holdout_ratio,
                                                   shuffle=dataset.val_col_shuffle, random_state=0)
            i_train = X[X[val_col].isin(val_train)].index.values
            i_test = X[X[val_col].isin(val_test)].index.values
        else:
            # test set generated by split with holdout ratio
            i_split = list(range(len(data_train)))
            if dataset.problem_type == 'regression':
                i_train, i_test = train_test_split(i_split, test_size=dataset.holdout_ratio,
                                                   shuffle=dataset.val_col_shuffle, random_state=0)
            else:
                try:
                    i_train, i_test = train_test_split(i_split, test_size=dataset.holdout_ratio,
                                                       shuffle=dataset.val_col_shuffle,
                                                       stratify=y, random_state=0)
                except:
                    # may fail if two few classes -> split without stratify
                    i_train, i_test = train_test_split(i_split, test_size=dataset.holdout_ratio,
                                                       shuffle=dataset.val_col_shuffle, random_state=0)
        X_train, X_test, y_train, y_test = X.iloc[i_train], X.iloc[i_test], y[i_train], y[i_test]

    # submit data
    if dataset.filename_submit != '':
        df = dataset.get_data('submit')
        if fe != '':
            df = apply_feature_engineering(dataset.dataset_id, df)
        X_submit = df[dataset.x_cols]
        id_submit = df[dataset.col_submit].values
    else:
        X_submit = []
        id_submit = []

    return X, y, X_train, X_test, y_train, y_test, X_submit, id_submit, i_train, i_test


def __create_cv(dataset, X, y):
    # generate cv folds
    val_col = dataset.val_col
    if val_col != 'index':
        skf = KFold(n_splits=dataset.cv_folds, shuffle=dataset.val_col_shuffle, random_state=0)
        val_values = np.sort(X[val_col].unique())
        cv_val = [(val_values[train_index], val_values[eval_index]) for train_index, eval_index in
                  skf.split(val_values)]
        cv_folds = [(X[X[val_col].isin(val_train)].index.values, X[X[val_col].isin(val_test)].index.values) for
                    val_train, val_test in cv_val]
    else:
        if dataset.problem_type == 'classification':
            skf = StratifiedKFold(n_splits=dataset.cv_folds, shuffle=dataset.val_col_shuffle, random_state=0)
            cv_folds = [(train_index, eval_index) for train_index, eval_index in skf.split(X, y)]
        else:
            skf = KFold(n_splits=dataset.cv_folds, shuffle=dataset.val_col_shuffle, random_state=0)
            cv_folds = [(train_index, eval_index) for train_index, eval_index in skf.split(X)]
    return cv_folds


def __store_eval_set(dataset, y_train, y_test, cv_folds):
    y_eval_list = []
    i_eval_list = []
    # stores eval set
    for i, (train_index, eval_index) in enumerate(cv_folds):
        pickle.dump(y_train[train_index], open(get_dataset_folder(dataset.dataset_id) + '/y_train_%d.pkl' % i, 'wb'))
        pickle.dump(y_train[eval_index], open(get_dataset_folder(dataset.dataset_id) + '/y_eval_%d.pkl' % i, 'wb'))
        y_eval_list.append(y_train[eval_index])
        i_eval_list.append(eval_index)

    # stores test set
    pickle.dump(y_test, open(get_dataset_folder(dataset.dataset_id) + '/y_test.pkl', 'wb'))

    # generate y_eval
    y_eval = np.concatenate(y_eval_list, axis=0)

    # store y_eval
    pickle.dump(y_train, open(get_dataset_folder(dataset.dataset_id) + '/y_eval.pkl', 'wb'))

    return y_eval_list, y_eval, np.concatenate(i_eval_list, axis=0), i_eval_list[0]


def __prepare_y(dataset, y, y_train, y_test):
    # pre-processing of y: categorical
    if dataset.problem_type == 'classification':
        # encode class values as integers
        map_y = {str(x): i for i, x in enumerate(dataset.y_class_names)}
        y = np.array([map_y[str(x)] if str(x) in map_y else 0 for x in y])
        y_train = np.array([map_y[str(x)] if str(x) in map_y else 0 for x in y_train])
        y_test = np.array([map_y[str(x)] if str(x) in map_y else 0 for x in y_test])
    return y, y_train, y_test
