import time, datetime, uuid, sys, random, socket, gc, os
from sklearn.model_selection import train_test_split, KFold
from sklearn.preprocessing import LabelEncoder
from .models import *
from .preprocessing import pre_processing
from .context import HyperContext
from .dataset import get_dataset
from .graphs import graph_pred_histogram, graph_predict

def worker_search(dataset_uid, search_mode='auto', max_iter=500):
    """
    launch the search of the best preprocessing + models
    :param dataset_uid: id of the dataset
    :param search_mode: options are : 'default' = default parameters, 'random' = random search, 'auto' = default, then random
    'ensemble' = ensemble models (requires a minimum of models to be searched in random/auto mode before)
    :param max_iter: maximum number of searches
    :return: None
    """
    dataset = get_dataset(dataset_uid)

    # create train & test set
    X_train, X_test, y_train, y_test = __create_train_test(dataset)

    # create cv folds
    cv_folds = __create_cv(dataset, X_train)

    # prepare y values
    y_train, y_test = __prepare_y(dataset, y_train, y_test)

    # prepare and store eval set
    y_eval_list, y_eval, i_eval = __store_eval_set(dataset, y_train, y_test, cv_folds)

    # optimize
    __launch_search(dataset, X_train, y_train, X_test, y_test, y_eval_list, y_eval, i_eval, cv_folds,
                    search_mode=search_mode,
                    max_iter=max_iter)


def get_y_eval(uid):
    # retrieves eval set
    return pickle.load(open(get_dataset_folder(uid) + '/y_eval.pkl', 'rb'))


def get_search_rounds(uid):
    # return search logs as a dataframe
    with open(get_dataset_folder(uid) + '/search.txt', 'r') as f:
        lines = f.readlines()

    results = []
    for line in lines:
        results.append(eval(line))

    return pd.DataFrame(results)


def __create_train_test(dataset):
    print('loading train set')
    dataset = get_dataset(dataset.uid)
    data_train = dataset.get_data()
    # TODO: split accornding to val_col
    # split into train & test set
    if dataset.with_test_set:
        print('loading test set')
        data_test = dataset.get_data('test')

        X_train = data_train[dataset.x_cols]
        y_train = data_train[dataset.y_col].values

        X_test = data_test[dataset.x_cols]
        y_test = data_test[dataset.y_col].values
    else:
        X = data_train[dataset.x_cols]
        y = data_train[dataset.y_col].values

        # test set generated by split with holdout ratio
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=dataset.holdout_ratio,
                                                            shuffle=dataset.val_col_shuffle, random_state=0)
    return X_train, X_test, y_train, y_test


def __create_cv(dataset, X_train):
    # generate cv folds
    # TODO: split according to val_col
    skf = KFold(n_splits=dataset.cv_folds, shuffle=dataset.val_col_shuffle, random_state=0)
    cv_folds = [(train_index, eval_index) for train_index, eval_index in skf.split(X_train)]
    return cv_folds


def __launch_search(dataset, X_train_ini, y_train_ini, X_test_ini, y_test_ini, y_eval_list, y_eval, i_eval, cv_folds,
                    search_mode='auto', max_iter=500):
    # performs a random search on all models

    print('hyper optimizing...')
    for i in range(max_iter):

        t_start = time.time()
        context = HyperContext(dataset.problem_type, dataset.x_cols, dataset.cat_cols, dataset.missing_cols)
        context, X_train, y_train, X_test, y_test = pre_processing(context, X_train_ini, y_train_ini, X_test_ini,
                                                                   y_test_ini)
        process_steps = {p.process_name: p.params for p in context.pipeline}
        t_end = time.time()
        duration_process = int(t_end - t_start)
        print('preprocessing steps:', process_steps)

        t = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        # TODO: mode focus = overweight choice of best models after some rounds
        try:
            if search_mode == 'default':
                model = __get_default_model(dataset, context)
                if not model:
                    return
            elif search_mode == 'random':
                model = __get_random_model(dataset, context)
            elif search_mode == 'ensemble':
                model, pool = __get_ensemble_model(dataset, context)
            elif search_mode == 'auto':
                # search in default mode, then random
                model = __get_default_model(dataset, context)
                if not model:
                    model = __get_random_model(dataset, context)

            # check if model library is loaded
            if model.model_loaded:
                print('optimizing with %s, params: %s' % (model.model_name, model.params))
                # fit, test & score
                t_start = time.time()
                if search_mode == 'ensemble':
                    y_pred_eval_list, y_pred_test_list = model.cv_pool(pool, y_train, y_test, cv_folds)
                else:
                    y_pred_eval_list, y_pred_test_list = model.cv(X_train, y_train, X_test, y_test, cv_folds)

                # print('eval list:', np.shape(y_pred_eval_list))
                # print('test list', np.shape(y_pred_test_list))

                # y_pred_eval as concat of folds
                y_pred_eval = np.concatenate(y_pred_eval_list)

                # reindex eval to be aligned with y
                y_pred_eval[i_eval] = y_pred_eval.copy()

                # mean of y_pred_test on multiple folds
                y_pred_test = np.mean(y_pred_test_list, axis=0)

                """
                print('y eval:', np.shape(y_eval), y_eval[:10])
                print('y pred eval:', np.shape(y_pred_eval), np.shape(y_pred_eval), y_pred_eval[:10])
                print('y pred test', np.shape(y_pred_test), y_pred_test[:10])
                """
                # save model importance, prediction and model
                model.save_importance()
                model.save_predict(y_pred_eval, y_pred_test)
                model.save_model()

                # generate graphs
                graph_predict(dataset, model.uuid, y_train, y_pred_eval, 'eval')
                graph_predict(dataset, model.uuid, y_test, y_pred_test, 'test')
                graph_pred_histogram(dataset.uid, model.uuid, y_pred_eval, 'eval')
                graph_pred_histogram(dataset.uid, model.uuid, y_pred_test, 'test')

                error = False
            else:
                error = True

        except KeyboardInterrupt:
            print('finishing search')
            return
        except ValueError as e:
            error = True
            __store_search_error(dataset, t, e, model)
        except Exception as e:
            error = True
            __store_search_error(dataset, t, e, model)

        if not error:
            # score on full eval set, test set and cv
            score_eval = dataset.evaluate_metric(y_train, y_pred_eval)
            score_test = dataset.evaluate_metric(y_test, y_pred_test)
            scores_cv = [dataset.evaluate_metric(y_act, y_pred) for y_act, y_pred in zip(y_eval_list, y_pred_eval_list)]
            cv_mean = np.mean(scores_cv)
            cv_std = np.std(scores_cv)

            # score with secondary metrics
            eval_other_metrics = {m: dataset.evaluate_metric(y_train, y_pred_eval, m) for m in dataset.other_metrics}
            test_other_metrics = {m: dataset.evaluate_metric(y_test, y_pred_test, m) for m in dataset.other_metrics}

            t_end = time.time()
            duration = int(t_end - t_start)

            # track search and results
            __store_search_round(dataset, t, duration_process, process_steps, duration, score_eval, score_test,
                                 scores_cv,
                                 cv_mean, cv_std,
                                 eval_other_metrics, test_other_metrics, model, search_mode)
            print(
                '%s duration:%ds, eval:%.5f, test:%.5f, cv:%.5f +/- %.5f, other metrics: eval=%s test=%s, rounds: %d, id: %s \n' % (
                    t, duration, score_eval, score_test, cv_mean, cv_std, eval_other_metrics,
                    test_other_metrics, model.num_rounds, model.uuid))

            # input('type enter to continue')

            del model
            gc.collect()


def __store_eval_set(dataset, y_train, y_test, cv_folds):
    y_eval_list = []
    i_eval_list = []
    # stores eval set
    for i, (train_index, eval_index) in enumerate(cv_folds):
        pickle.dump(y_train[train_index], open(get_dataset_folder(dataset.uid) + '/y_train_%d.pkl' % i, 'wb'))
        pickle.dump(y_train[eval_index], open(get_dataset_folder(dataset.uid) + '/y_eval_%d.pkl' % i, 'wb'))
        y_eval_list.append(y_train[eval_index])
        i_eval_list.append(eval_index)

    # stores test set
    pickle.dump(y_test, open(get_dataset_folder(dataset.uid) + '/y_test.pkl', 'wb'))

    # generate y_eval
    y_eval = np.concatenate(y_eval_list, axis=0)

    # store y_eval
    pickle.dump(y_train, open(get_dataset_folder(dataset.uid) + '/y_eval.pkl', 'wb'))

    return y_eval_list, y_eval, np.concatenate(i_eval_list, axis=0)


def __store_search_round(dataset, t, duration_process, process_steps, duration, score_eval, score_test, scores_cv,
                         cv_mean, cv_std, eval_other_metrics,
                         test_other_metrics, model, search_mode):
    # track score
    with open(get_dataset_folder(dataset.uid) + '/search.txt', 'a') as f:
        s = "{'time':'%s', 'duration_process':'%.2f', 'duration':'%.2f', 'score_eval':%.6f, 'score_test':%.6f, 'scores_cv': %s, 'cv_mean':%.6f, 'cv_std':%.6f, 'eval_other_metrics': %s, 'test_other_metrics': %s, 'model_class': '%s', 'model_level': %d, 'model': '%s', 'params': %s, 'rounds': %d, 'process': %s, 'uuid': '%s', 'host': '%s', 'search_mode': '%s'}" % (
            t, duration_process, duration, score_eval, score_test, scores_cv, cv_mean, cv_std, eval_other_metrics,
            test_other_metrics, model.__class__.__name__, model.model_level, model.model_name, model.params,
            model.num_rounds, process_steps, model.uuid, socket.gethostname(), search_mode)

        f.write(s + '\n')
        print(s)


def __store_search_error(dataset, t, e, model):
    print('Error: ', e)
    # track error
    with open(get_dataset_folder(dataset.uid) + '/errors.txt', 'a') as f:
        f.write("'time':'%s', 'model': %s, 'params': %s, '\n Error': %s \n" % (
            t, model.model_name, model.params, str(e)))


def __get_model_class_list(dataset):
    # generates the list of potential models depending on the problem type

    # common models for regresion & classification
    choices = [HyperModelLightGBM,
               HyperModelXgBoost,
               HyperModelCatboost,
               HyperModelNN,
               HyperModelExtraTrees,
               HyperModelRandomForest,
               HyperModelGradientBoosting,
               HyperModelAdaBoost,
               HyperModelSVM,
               HyperModelKnn]

    if dataset.problem_type == 'regression':
        return choices + [HyperModelLinearRegressor, HyperModelLinearSVR]
    else:  # classification
        return choices + [HyperModelLogisticRegression]


def __get_ensemble_class_list(dataset):
    # generates the list of potential models depending on the problem type
    choices = [HyperModelEnsembleSelection,
               HyperModelStackingLightGBM,
               HyperModelStackingXgBoost,
               # HyperModelStackingNN,
               HyperModelStackingExtraTrees,
               HyperModelStackingGradientBoosting,
               HyperModelStackingRandomForest]

    if dataset.problem_type == 'regression':
        return choices + [HyperModelStackingLinear]
    else:  # classification
        return choices + [HyperModelStackingLogistic]


def __get_default_model(dataset, context):
    # generates a default model to evaluate which has not been tested yet

    # retrieves the list of default models already searched
    default_file_name = get_dataset_folder(dataset.uid) + '/default.txt'
    if os.path.isfile(default_file_name):
        with open(default_file_name, 'r') as f:
            searched_list = [line.rstrip() for line in f]
    else:
        searched_list = []

    # find a model not already in in the list
    choices = __get_model_class_list(dataset)

    remaining_choices = [model_class for model_class in choices if model_class.__name__ not in searched_list]

    if len(remaining_choices) == 0:
        return None

    # take the first model in the remaining list
    model_class = remaining_choices[0]
    model = model_class(dataset, context)
    model.set_default_params()
    model.set_model()

    # mark the file as reserved
    with open(default_file_name, 'a') as f:
        f.write(model_class.__name__ + '\n')

    return model


def __get_random_model(dataset, context):
    # generates a random model with random parameters
    choices = __get_model_class_list(dataset)
    model_class = random.choice(choices)
    model = model_class(dataset, context)
    if model.model_loaded:
        model.set_random_params()
        model.set_model()
    return model


def __get_pool_models(dataset):
    # retrieves all results in order to build and ensemble
    df = get_search_rounds(dataset.uid)

    # keep only models of level 0
    df = df[(df.model_level == 0) & (df.score_eval != METRIC_NULL)]
    print('length of pool for ensemble', len(df))

    model_uuids = df.uuid.values
    model_names = df.model.values

    # retrieves predictions
    preds = [get_pred_eval_test(dataset.uid, uuid) for uuid in model_uuids]
    preds_eval = [x[0] for x in preds]
    preds_test = [x[1] for x in preds]

    return EnsemblePool(model_uuids, model_names, preds_eval, preds_test)


def __get_ensemble_model(dataset, context):
    # generates an ensemble model to evaluate which has not been tested yet
    choices = __get_ensemble_class_list(dataset)
    model_class = random.choice(choices)
    model = model_class(dataset, context)
    pool = __get_pool_models(dataset)
    if model.model_loaded:
        model.set_random_params()
        model.set_model()
    return model, pool


def __prepare_y(dataset, y_train, y_test):
    # pre-processing of y: categorical
    if dataset.problem_type == 'classification':
        print('y label encoding')
        # encode class values as integers
        encoder = LabelEncoder()
        y_train = encoder.fit_transform(y_train)
        y_test = encoder.transform(y_test)
    return y_train, y_test
