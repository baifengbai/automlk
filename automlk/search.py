import datetime
import socket
import gc
import os
from sklearn.model_selection import train_test_split, KFold, StratifiedKFold
from sklearn.preprocessing import LabelEncoder
from .models import *
from .preprocessing import pre_processing
from .context import HyperContext
from .dataset import get_dataset
from .graphs import graph_pred_histogram, graph_predict


PATIENCE_RANDOM = 100
PATIENCE_ENSEMBLE = 100


def worker_search(dataset_uid, search_mode='auto', max_iter=500):
    """
    launch the search of the best preprocessing + models

    :param dataset_uid: id of the dataset
    :param search_mode: options are : 'default' = default parameters, 'random' = random search, 'auto' = default, then random
    'ensemble' = ensemble models (requires a minimum of models to be searched in random/auto mode before)
    :param max_iter: maximum number of searches
    """
    dataset = get_dataset(dataset_uid)

    # create train & test set
    X_train, X_test, y_train, y_test = __create_train_test(dataset)

    # prepare y values
    y_train, y_test = __prepare_y(dataset, y_train, y_test)

    # create cv folds
    cv_folds = __create_cv(dataset, X_train, y_train)

    # prepare and store eval set
    y_eval_list, y_eval, idx_eval = __store_eval_set(dataset, y_train, y_test, cv_folds)

    # optimize
    __launch_search(dataset, X_train, y_train, X_test, y_test, y_eval_list, y_eval, idx_eval, cv_folds,
                    search_mode=search_mode,
                    max_iter=max_iter)


def get_search_rounds(uid):
    """
    get all the results of the search with preprocessing and models

    :param uid: id of the dataset
    :return: results of the search as a dataframe
    """
    # return search logs as a dataframe
    with open(get_dataset_folder(uid) + '/search.txt', 'r') as f:
        lines = f.readlines()

    results = []
    for line in lines:
        results.append(eval(line))

    return pd.DataFrame(results)


def get_y_eval(uid):
    """
    retrieves the y value of the eval set

    :param uid: id of the dataset
    :return: y values
    """
    #
    return pickle.load(open(get_dataset_folder(uid) + '/y_eval.pkl', 'rb'))


def __create_train_test(dataset):
    print('loading train set')
    dataset = get_dataset(dataset.uid)
    data_train = dataset.get_data()
    # TODO: split according to val_col
    # split into train & test set
    if dataset.with_test_set:
        print('loading test set')
        data_test = dataset.get_data('test')

        X_train = data_train[dataset.x_cols]
        y_train = data_train[dataset.y_col].values

        X_test = data_test[dataset.x_cols]
        y_test = data_test[dataset.y_col].values
    else:
        X = data_train[dataset.x_cols]
        y = data_train[dataset.y_col].values

        # test set generated by split with holdout ratio
        if dataset.problem_type == 'regression':
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=dataset.holdout_ratio,
                                                                shuffle=dataset.val_col_shuffle, random_state=0)
        else:
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=dataset.holdout_ratio,
                                                                shuffle=dataset.val_col_shuffle,
                                                                stratify=y,
                                                                random_state=0)
    return X_train, X_test, y_train, y_test


def __create_cv(dataset, X_train, y_train):
    # generate cv folds
    # TODO: split according to val_col
    if dataset.problem_type == 'classification':
        skf = StratifiedKFold(n_splits=dataset.cv_folds, shuffle=dataset.val_col_shuffle, random_state=0)
        cv_folds = [(train_index, eval_index) for train_index, eval_index in skf.split(X_train, y_train)]
    else:
        skf = KFold(n_splits=dataset.cv_folds, shuffle=dataset.val_col_shuffle, random_state=0)
        cv_folds = [(train_index, eval_index) for train_index, eval_index in skf.split(X_train)]
    return cv_folds


def __launch_search(dataset, X_train_ini, y_train_ini, X_test_ini, y_test_ini, y_eval_list, y_eval, i_eval, cv_folds,
                    search_mode='auto', max_iter=500):
    # performs a random search on all models

    print('hyper optimizing...')
    for i in range(max_iter):

        # get search history
        df = get_search_rounds(dataset.uid)

        # get outlier threshold of metrics
        if len(df) > 10:
            threshold = __get_outlier_threshold(df)
        else:
            threshold = 0

        t = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        t_start = time.time()
        context = HyperContext(dataset.problem_type, dataset.x_cols, dataset.cat_cols, dataset.missing_cols)

        context, X_train, y_train, X_test, y_test = pre_processing(context, X_train_ini, y_train_ini, X_test_ini,
                                                                   y_test_ini)

        process_steps = {p.process_name: p.params for p in context.pipeline}
        t_end = time.time()
        duration_process = int(t_end - t_start)
        print('preprocessing steps:', process_steps)

        # TODO: mode focus = overweight choice of best models after some rounds
        #try:
        level = 0
        if search_mode == 'default':
            model = __get_default_model(dataset, context)
            if not model:
                return
        elif search_mode == 'random':
            model = __get_random_model(dataset, context)
        elif search_mode == 'ensemble':
            ensemble_depth = random.randint(1, 10)
            model, pool = __get_ensemble_model(dataset, context, ensemble_depth)
            level = 1
        elif search_mode == 'auto':
            # search in default mode, then random
            model = __get_default_model(dataset, context)
            if not model:
                best, i_best = __get_last_best(df, level=0)
                if len(df) - i_best > PATIENCE_RANDOM:
                    # ensemble search
                    level = 1
                    search_mode = 'ensemble'
                    best, i_best = __get_last_best(df, level=1)
                    if i_best > 0 and len(df) - i_best > PATIENCE_ENSEMBLE:
                        print('ensemble search complete - patience reached')
                        return
                    ensemble_depth = random.randint(1, 10)
                    model, pool = __get_ensemble_model(dataset, context, ensemble_depth)
                else:
                    # random search
                    model = __get_random_model(dataset, context)

        # check if model library is loaded
        if model.model_loaded:
            print('optimizing with %s, params: %s' % (model.model_name, model.params))
            # fit, test & score
            t_start = time.time()
            if level == 1:
                outlier, y_pred_eval_list, y_pred_test_list = model.cv_pool(pool, y_train, y_test, cv_folds, threshold, ensemble_depth)
            else:
                outlier, y_pred_eval_list, y_pred_test_list = model.cv(X_train, y_train, X_test, y_test, cv_folds, threshold)

            # check outlier
            if outlier:
                print('outlier, skipping this round')
                continue

            # y_pred_eval as concat of folds
            y_pred_eval = np.concatenate(y_pred_eval_list)

            # reindex eval to be aligned with y
            y_pred_eval[i_eval] = y_pred_eval.copy()

            # mean of y_pred_test on multiple folds
            y_pred_test = np.mean(y_pred_test_list, axis=0)

            # save model importance, prediction and model
            model.save_importance()
            model.save_predict(y_pred_eval, y_pred_test)
            model.save_model()

            # generate graphs
            graph_predict(dataset, model.uuid, y_train, y_pred_eval, 'eval')
            graph_predict(dataset, model.uuid, y_test, y_pred_test, 'test')
            graph_pred_histogram(dataset.uid, model.uuid, y_pred_eval, 'eval')
            graph_pred_histogram(dataset.uid, model.uuid, y_pred_test, 'test')

            error = False
        else:
            error = True
        """
        except KeyboardInterrupt:
            print('finishing search')
            return
        except ValueError as e:
            error = True
            __store_search_error(dataset, t, e, model)
        except Exception as e:
            error = True
            print(e)
        """
        if not error:
            # score on full eval set, test set and cv
            score_eval = dataset.evaluate_metric(y_train, y_pred_eval)
            score_test = dataset.evaluate_metric(y_test, y_pred_test)
            scores_cv = [dataset.evaluate_metric(y_act, y_pred) for y_act, y_pred in zip(y_eval_list, y_pred_eval_list)]
            cv_mean = np.mean(scores_cv)
            cv_std = np.std(scores_cv)

            # score with secondary metrics
            eval_other_metrics = {m: dataset.evaluate_metric(y_train, y_pred_eval, m) for m in dataset.other_metrics}
            test_other_metrics = {m: dataset.evaluate_metric(y_test, y_pred_test, m) for m in dataset.other_metrics}

            t_end = time.time()
            duration = int(t_end - t_start)

            # track search and results
            __store_search_round(dataset, t, duration_process, process_steps, duration, score_eval, score_test,
                                 scores_cv,
                                 cv_mean, cv_std,
                                 eval_other_metrics, test_other_metrics, model, search_mode)
            print(
                '%s duration:%ds, eval:%.5f, test:%.5f, cv:%.5f +/- %.5f, other metrics: eval=%s test=%s, rounds: %d, id: %s \n' % (
                    t, duration, score_eval, score_test, cv_mean, cv_std, eval_other_metrics,
                    test_other_metrics, model.num_rounds, model.uuid))

            del model
            gc.collect()


def __get_outlier_threshold(df):
    # calculates the threshold for outliers from the score history
    # TODO: adapt threshold with number of [successful] rounds
    df0 = df[(df.score_eval != METRIC_NULL) & (df.model_level == 0)].groupby('model', as_index=False).min()
    score_mean = df0.score_eval.mean()
    score_std = df0.score_eval.std()
    print('outlier threshold set at: %.5f (mean: %.5f, std: %.5f)' % (
    abs(score_mean + 0.5 * score_std), abs(score_mean), abs(score_std)))
    return score_mean + 3 * score_std


def __get_last_best(df, level):
    # returns last best value and its index
    best = METRIC_NULL
    i_best = -1
    for i, score in enumerate(df[df.model_level==level].score_eval.values):
        if score < best:
            best = score
            i_best = i
    return best, i_best


def __store_eval_set(dataset, y_train, y_test, cv_folds):
    y_eval_list = []
    i_eval_list = []
    # stores eval set
    for i, (train_index, eval_index) in enumerate(cv_folds):
        pickle.dump(y_train[train_index], open(get_dataset_folder(dataset.uid) + '/y_train_%d.pkl' % i, 'wb'))
        pickle.dump(y_train[eval_index], open(get_dataset_folder(dataset.uid) + '/y_eval_%d.pkl' % i, 'wb'))
        y_eval_list.append(y_train[eval_index])
        i_eval_list.append(eval_index)

    # stores test set
    pickle.dump(y_test, open(get_dataset_folder(dataset.uid) + '/y_test.pkl', 'wb'))

    # generate y_eval
    y_eval = np.concatenate(y_eval_list, axis=0)

    # store y_eval
    pickle.dump(y_train, open(get_dataset_folder(dataset.uid) + '/y_eval.pkl', 'wb'))

    return y_eval_list, y_eval, np.concatenate(i_eval_list, axis=0)


def __store_search_round(dataset, t, duration_process, process_steps, duration, score_eval, score_test, scores_cv,
                         cv_mean, cv_std, eval_other_metrics,
                         test_other_metrics, model, search_mode):
    # track score
    with open(get_dataset_folder(dataset.uid) + '/search.txt', 'a') as f:
        s = "{'time':'%s', 'duration_process':'%.2f', 'duration':'%.2f', 'score_eval':%.6f, 'score_test':%.6f, 'scores_cv': %s, 'cv_mean':%.6f, 'cv_std':%.6f, 'eval_other_metrics': %s, 'test_other_metrics': %s, 'model_class': '%s', 'model_level': %d, 'model': '%s', 'params': %s, 'rounds': %d, 'process': %s, 'uuid': '%s', 'host': '%s', 'search_mode': '%s'}" % (
            t, duration_process, duration, score_eval, score_test, scores_cv, cv_mean, cv_std, eval_other_metrics,
            test_other_metrics, model.__class__.__name__, model.model_level, model.model_name, model.params,
            model.num_rounds, process_steps, model.uuid, socket.gethostname(), search_mode)

        f.write(s + '\n')
        print(s)


def __store_search_error(dataset, t, e, model):
    print('Error: ', e)
    # track error
    with open(get_dataset_folder(dataset.uid) + '/errors.txt', 'a') as f:
        f.write("'time':'%s', 'model': %s, 'params': %s, '\n Error': %s \n" % (
            t, model.model_name, model.params, str(e)))


def __get_model_class_list(dataset):
    # generates the list of potential models depending on the problem type

    # common models for regresion & classification
    choices = [HyperModelLightGBM,
               HyperModelXgBoost,
               HyperModelCatboost,
               HyperModelNN,
               HyperModelExtraTrees,
               HyperModelRandomForest,
               HyperModelGradientBoosting,
               HyperModelAdaBoost,
               HyperModelKnn]

    if dataset.n_rows < 2000:
        # we limit SVM kernel to small datasets, because it takes quadratic time of dataset size
        choices += [HyperModelSVM]

    if dataset.problem_type == 'regression':
        return choices + [HyperModelLinearRegressor, HyperModelLinearSVR, HyperModelLassoRegressor,
                          HyperModelRidgeRegressor, HyperModelHuberRegressor]
    else:  # classification
        return choices + [HyperModelLogisticRegression]


def __get_ensemble_class_list(dataset):
    # generates the list of potential models depending on the problem type
    choices = [HyperModelEnsembleSelection,
               HyperModelStackingLightGBM,
               HyperModelStackingXgBoost,
               # HyperModelStackingNN,
               HyperModelStackingExtraTrees,
               HyperModelStackingGradientBoosting,
               HyperModelStackingRandomForest]

    if dataset.problem_type == 'regression':
        return choices + [HyperModelStackingLinear]
    else:  # classification
        return choices + [HyperModelStackingLogistic]


def __get_default_model(dataset, context):
    # generates a default model to evaluate which has not been tested yet

    # retrieves the list of default models already searched
    default_file_name = get_dataset_folder(dataset.uid) + '/default.txt'
    if os.path.isfile(default_file_name):
        with open(default_file_name, 'r') as f:
            searched_list = [line.rstrip() for line in f]
    else:
        searched_list = []

    # find a model not already in in the list
    choices = __get_model_class_list(dataset)

    remaining_choices = [model_class for model_class in choices if model_class.__name__ not in searched_list]

    if len(remaining_choices) == 0:
        return None

    # take the first model in the remaining list
    model_class = remaining_choices[0]
    model = model_class(dataset, context)
    model.set_default_params()
    model.set_model()

    # mark the file as reserved
    with open(default_file_name, 'a') as f:
        f.write(model_class.__name__ + '\n')

    return model


def __get_random_model(dataset, context):
    # generates a random model with random parameters
    choices = __get_model_class_list(dataset)
    model_class = random.choice(choices)
    model = model_class(dataset, context)
    if model.model_loaded:
        model.set_random_params()
        model.set_model()
    return model


def __get_pool_models(dataset, depth):
    # retrieves all results in order to build and ensemble
    df = get_search_rounds(dataset.uid)

    # keep only the first (depth) models of level 0
    df = df[(df.model_level == 0) & (df.score_eval != METRIC_NULL)].sort_values(by=['model', 'score_eval'])
    model_uuids = []
    model_names = []
    k_model = ''
    for index, row in df.iterrows():
        if k_model != row['model']:
            count_model = 0
            k_model = row['model']
        if count_model > depth:
            continue
        model_names.append(row['model'])
        model_uuids.append(row['uuid'])
        count_model += 1

    print('length of pool: %d for ensemble of depth %d' % (len(model_uuids), depth))
    # retrieves predictions
    preds = [get_pred_eval_test(dataset.uid, uuid) for uuid in model_uuids]
    preds_eval = [x[0] for x in preds]
    preds_test = [x[1] for x in preds]

    return EnsemblePool(model_uuids, model_names, preds_eval, preds_test)


def __get_ensemble_model(dataset, context, depth):
    # generates an ensemble model to evaluate which has not been tested yet
    choices = __get_ensemble_class_list(dataset)
    model_class = random.choice(choices)
    model = model_class(dataset, context)
    pool = __get_pool_models(dataset, depth)
    if model.model_loaded:
        model.set_random_params()
        model.set_model()
    return model, pool


def __prepare_y(dataset, y_train, y_test):
    # pre-processing of y: categorical
    if dataset.problem_type == 'classification':
        print('y label encoding')
        # encode class values as integers
        encoder = LabelEncoder()
        encoder.fit([str(x) for x in np.concatenate((y_train,y_test), axis=0)])
        y_train = encoder.transform([str(x) for x in y_train])
        y_test = encoder.transform([str(x) for x in y_test])
    return y_train, y_test
